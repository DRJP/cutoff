% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/em.R
\name{em}
\alias{em}
\title{Expectation-Maximization estimation of a finite mixture model}
\usage{
em(data, D1, D2, t = 1e-64, penaltyScale = 0, forceOrdering = FALSE)
}
\arguments{
\item{data}{A vector of real numbers, the data to model with a finite mixture model.}

\item{D1}{First probability distribution in the finite mixture model.}

\item{D2}{Second probability distribution in the finite mixture model. See Details.}

\item{t}{A numerical scalar indicating the value below which the E-M algorithm should stop.}

\item{penaltyScale}{A positive scale parameter to penalise biologically nonsensical solutions. Defaults to 0.}

\item{forceOrdering}{Logical. If TRUE then models 1 and 2 are switched every time the maximisation step results in mu[1] > mu[2]. A warning message is printed each time this occurs.}
}
\value{
A list with class \code{em} containing the following components:
	\item{lambda}{a numerical vector of length \code{length(data)} containing, for each datum, the probability to belong to distribution \code{D1}.}
 \item{param}{the location (mu) and scale (sigma) parameters of the probability distributions \code{D1} and \code{D2}.}
	\item{D1}{character scalar containing the name of the first probability distribution used in the finite mixture model.}
	\item{D2}{character scalar containing the name of the second probability distribution used in the finite mixture model.}
 \item{deviance}{scalar value giving the devinace of the fit. Set in cutoff2 as twice the -ve log-likelihood (this doubling was omitted in cutoff). }
 \item{data}{the numerical vector of data used as input.}
	\item{data_name}{character scalar containing the name of the dataset used as input.}
	\item{out}{an object of class \code{mle2} that contains the maximum-likelihood estimates of parameters \code{mu1}, \code{lambda1}},
	\item{t}{the input \code{t} argument value.}
}
\description{
\code{em} returns points estimations of the parameters of a finite mixture
	model using the Expectation-Maximization (E-M) algorithm.
}
\details{
The finite mixture model considered in this function is a mixture of two
 probability distributions that are one of the following: normal, log-normal,
 gamma or Weibull. Each of these distributions is defined by two parameters:
 a location and a scale parameter:
\tabular{lcc}{
             \tab location \tab scale \cr
  normal     \tab mean     \tab sd    \cr
  log-normal \tab meanlog  \tab sdlog \cr
  gamma      \tab shape    \tab rate  \cr
  Weibull    \tab shape    \tab scale
}
These parameters, together with the mixture parameter, are estimated by the
 Expection-Maximization algorithm.
}
\examples{
# Measles IgG concentration data:
length(measles)
range(measles)
# Plotting the data:
hist(measles,100,FALSE,xlab="concentration",ylab="density",ylim=c(0,.55), main=NULL,col="grey")
# The kernel density:
lines(density(measles),lwd=1.5,col="blue")
# Estimating the parameters of the finite mixture model:
(measles_out <- em(measles,"normal","normal"))
# The confidence interval of the parameter estimates:
confint(measles_out,t=1e-64,nb=100,level=.95)
# Adding the E-M estimated finite mixture model:
lines(measles_out,lwd=1.5,col="red")
# The legend:
legend("topleft",leg=c("non-parametric","E-M"),col=c("blue","red"),
  lty=1,lwd=1.5,bty="n")

# Example 2: using penalisation to avoid curve 2 dominating curve 1 at low values of x
set.seed(4)
par(mfrow=c(2,2))
mu = c(4, 6)
sd = c(1, 4)
w  = c(0.5, 0.5)

#######################
## Plot 1: the model ##
#######################
xRange = range(qnorm(p=c(0.001,0.001,0.999,0.999),mu,sd))
xRange[1] = floor(xRange[1])
xRange[2] = ceiling(xRange[2])
curve(w[1]*dnorm(x,mu[1],sd[1]) + w[2]*dnorm(x,mu[2],sd[2]), xRange[1], xRange[2], n=1111,
lwd=2, ylab="Density" ,xlab="MFI")
curve(w[1]*dnorm(x,mu[1],sd[1]), xRange[1], xRange[2], n=1111, lty=2, lwd=2 ,add=TRUE, col="blue")
curve(w[2]*dnorm(x,mu[2],sd[2]), xRange[1], xRange[2], n=1111, lty=2, lwd=2, add=TRUE, col="red")
title("Biological nonsense model")
legend(legend=c("model", "pos","neg"),"topright", lwd=c(2,2,2), col=c("black","red","blue"),
bty="n" ,lty=c(1,2,2))
print(w)

######################
## Plot 2: the data ##
######################
n = 100
n1 = rbinom(1,n,w[1]); n2 = n - n1
y = c(rnorm(n1,mu[1],sd[1]), rnorm(n2,mu[2],sd[2]))
miny = floor(min(y))
maxy = ceiling(max(y))
hist(y, freq=FALSE, breaks=seq(miny, maxy, by=0.5), xlab="MFI", main="Simulated data")
curve(w[1]*dnorm(x,mu[1],sd[1])+w[2]*dnorm(x,mu[2],sd[2]),miny,maxy,add=TRUE,col="black", lwd=3)
curve(w[1]*dnorm(x,mu[1],sd[1]),miny,maxy,add=TRUE,col="black", lwd=2, lty=2)
curve(w[2]*dnorm(x,mu[2],sd[2]),miny,maxy,add=TRUE,col="black", lwd=2, lty=2)

#######################
## Unconstrained Fit ##
#######################
# Estimate parameters of finite mixture model:
(fit1 <- em(y,"normal","normal"))
hist(y, freq=FALSE, breaks=seq(miny, maxy, by=0.5), xlab="MFI", main="Unconstrained fit")
# Add the EM estimated finite mixture model:
lines(fit1, col="tomato", lwd=2)
# Estimate a cutoff from the fitted mixture model
(cut_off <- cutoff(fit1, whose="Titterington", nb=1000))
polygon(c(cut_off[-1],rev(cut_off[-1])),c(0,0,.55,.55), col=rgb(1, 0.39, 0.28,.2),border=NA)
abline(v=cut_off[-1],lty=2,col="tomato")
abline(v=cut_off[1],col="tomato")

###################
## Penalised fit ##
###################
# Estimate parameters of finite mixture model:
(fit2 <- em(y,"normal","normal", penaltyScale=1E4))
# Replot data
hist(y, freq=FALSE, breaks=seq(miny, maxy, by=0.5), xlab="MFI", main="Penalised fit")
# Add the penalised-EM estimated finite mixture model:
lines(fit2, col="red", lwd=2)
# Estimate a cutoff from the fitted mixture model
(cut_off <- cutoff(fit2, whose="Titterington", nb=1000))
polygon(c(cut_off[-1],rev(cut_off[-1])),c(0,0,.55,.55), col=rgb(1,0,0,.2),border=NA)
abline(v=cut_off[-1],lty=2,col="red")
abline(v=cut_off[1],col="red")

###################################
## Exploring different penalties ##
###################################

par(mfrow=c(2,3))
fit0 <- em(y,"normal","normal")
fit2 <- em(y,"normal","normal", penaltyScale=1E2)
fit4 <- em(y,"normal","normal", penaltyScale=1E4)
fit6 <- em(y,"normal","normal", penaltyScale=1E6)
fit8 <- em(y,"normal","normal", penaltyScale=1E8)
fit10 <- em(y,"normal","normal", penaltyScale=1E10)
plot(y, fit0$pPositive, main="penaltyScale=0", xlab="Serology data", ylab="p(positive)", ylim=0:1); abline(h=0)
plot(y, fit2$pPositive, main="penaltyScale=1E2", xlab="Serology data", ylab="p(positive)", ylim=0:1); abline(h=0)
plot(y, fit4$pPositive, main="penaltyScale=1E4", xlab="Serology data", ylab="p(positive)"); abline(h=0)
plot(y, fit6$pPositive, main="penaltyScale=1E6", xlab="Serology data", ylab="p(positive)"); abline(h=0)
plot(y, fit8$pPositive, main="penaltyScale=1E8", xlab="Serology data", ylab="p(positive)"); abline(h=0)
plot(y, fit10$pPositive, main="penaltyScale=1E10", xlab="Serology data", ylab="p(positive)"); abline(h=0)

}
\references{
Chuong B. Do and Serafim Batzoglou (2008) What is the expectation
   maximization algorithm? Nature Biotechnology 26(8): 897-899.\cr
 \cr
 Peter Schlattmann (2009) Medical Applications of Finite Mixture Models.
   Springer-Verlag, Berlin.
}
\seealso{
\code{\link{confint.em}} method for calculating the confidence
   intervals of the parameters and \code{\link{cutoff}} for deriving a
   cut-off value.
}
